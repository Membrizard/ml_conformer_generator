{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6301fa3-1fa7-4aac-b32d-7216418a4cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_conformer_generator import MLConformerGenerator\n",
    "from ml_conformer_generator.utils import get_context_shape\n",
    "from cheminformatics import evaluate_samples\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdDistGeom\n",
    "\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "\n",
    "\n",
    "def exact_match(mol, source):\n",
    "    sample_inchi = Chem.MolToInchi(mol)\n",
    "\n",
    "    with open(source, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            cid, inchi = line.replace(\"\\n\", \"\").split('\\t')\n",
    "            if sample_inchi == inchi:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "generator = MLConformerGenerator(device=device)\n",
    "source_path = \"\"\n",
    "n_samples = 100\n",
    "max_variance = 5\n",
    "\n",
    "references = []\n",
    "n_ref = len(reference)\n",
    "expected_n_samples = n_samples * n_ref\n",
    "\n",
    "node_dist_dict = dict()  # n_atoms : number of samples generated for mols with the given n_atoms\n",
    "variance_dist_dict = dict()  # n_atom variance from ref n_atoms : number of samples with such variance\n",
    "\n",
    "variance_context_errors = dict()  # n_atom variance from ref n_atoms : error in context\n",
    "ref_mol_size_context_errors = dict() # n_atoms: error in context\n",
    "\n",
    "variance_shape_tanimoto_scores = dict() # n_atom variance from ref n_atoms : shape tanimoto\n",
    "ref_mol_size_shape_tanimoto_scores = dict() # n_atoms: shape tanimoto\n",
    "\n",
    "variance_chem_tanimoto_scores = dict() # n_atom variance from ref n_atoms : chemical tanimoto\n",
    "ref_mol_size_chem_tanimoto_scores = dict() # n_atoms: chemical tanimoto\n",
    "\n",
    "ref_mol_size_valid = dict() # n_atoms: number of valid molecules generated in % of requested to be generated\n",
    "\n",
    "chem_unique_samples = 0 # number of chemically unique samples generated\n",
    "valid_samples = 0 # number of valid samples generated\n",
    "\n",
    "\n",
    "for i, reference in enumerate(references):\n",
    "    print(f\"Analysing samples for reference compound {i + 1} of {n_ref}\")\n",
    "\n",
    "    ref_n_atoms = reference.GetNumAtoms()\n",
    "    \n",
    "    samples = generator.generate_conformers(reference_conformer=ref_mol, n_samples=n_samples, variance=max_variance)\n",
    "    _, std_samples = evaluate_samples(ref_mol, samples)\n",
    "\n",
    "    valid_samples += len[std_samples]\n",
    "\n",
    "    # Log fraction of valid molecules\n",
    "    if ref_n_atoms in node_dist_dict.keys():\n",
    "        ref_mol_size_valid[ref_n_atoms] += len[std_samples] / n_samples\n",
    "    else:\n",
    "        ref_mol_size_valid[ref_n_atoms] = len[std_samples] / n_samples\n",
    "\n",
    "    \n",
    "    for std_sample in std_samples:\n",
    "        \n",
    "        sample_mol = Chem.MolFromMolBlock(std_sample['mol_block'])\n",
    "\n",
    "        # Check for sample uniqueness\n",
    "\n",
    "        if exact_match(sample_mol, source_path):\n",
    "            chem_unique_samples += 1\n",
    "\n",
    "        sample_num_atoms = sample_mol.GetNumAtoms()\n",
    "        variance = ref_n_atoms - sample_num_atoms  # -> Can be negative intentionally\n",
    "        sample_conformer = sample_mol.GetConformer()\n",
    "        sample_coord = torch.tensor(sample_conformer.GetPositions(), dtype=torch.float32)\n",
    "   \n",
    "        sample_context, _ = get_context_shape(sample_coord)\n",
    "        context_error = torch.abs(reference_context - sample_context)\n",
    "\n",
    "        # Log the error in context generation and tanimoto scores for ref_n_atoms\n",
    "        if ref_n_atoms in node_dist_dict.keys():\n",
    "            node_dist_dict[ref_n_atoms] += 1\n",
    "            ref_mol_size_context_errors[ref_n_atoms] += context_error\n",
    "            ref_mol_size_shape_tanimoto_scores[ref_n_atoms] += std_sample['shape_tanimoto']\n",
    "            ref_mol_size_chem_tanimoto_scores[ref_n_atoms] += std_sample['chemical_tanimoto']\n",
    "    \n",
    "        else:\n",
    "            node_dist_dict[ref_n_atoms] = 1\n",
    "            ref_mol_size_context_errors[ref_n_atoms] = context_error\n",
    "            ref_mol_size_shape_tanimoto_scores[ref_n_atoms] = std_sample['shape_tanimoto']\n",
    "            ref_mol_size_chem_tanimoto_scores[ref_n_atoms] = std_sample['chemical_tanimoto']\n",
    "\n",
    "        # Log the error in context generation and tanimoto scores for variance\n",
    "        if variance in variance_dist_dict.keys():\n",
    "            variance_dist_dict[variance] += 1\n",
    "            variance_context_errors[variance] += context_error\n",
    "            variance_shape_tanimoto_scores[ref_n_atoms] += std_sample['shape_tanimoto']\n",
    "            variance_chem_tanimoto_scores[ref_n_atoms] += std_sample['chemical_tanimoto']\n",
    "        else:\n",
    "            variance_dist_dict[variance] = 1\n",
    "            variance_context_errors[variance] = context_error\n",
    "            variance_shape_tanimoto_scores[ref_n_atoms] = std_sample['shape_tanimoto']\n",
    "            variance_chem_tanimoto_scores[ref_n_atoms] = std_sample['chemical_tanimoto']\n",
    "\n",
    "valid_samples_rate = valid_samples / expected_n_samples\n",
    "chem_unique_samples_rate = chem_unique_samples / valid_samples\n",
    "\n",
    "\n",
    "# Calculate mean Error and Tanimoto score values, for ref_mol_size and variance\n",
    "\n",
    "for key in node_dist_dict.keys():\n",
    "    ref_mol_size_context_errors[key] = ref_mol_size_context_errors[key] / node_dist_dict[key]\n",
    "    ref_mol_size_shape_tanimoto_scores[key] = ref_mol_size_tanimoto_scores[key] / node_dist_dict[key]\n",
    "    ref_mol_size_chem_tanimoto_scores[key] = ref_mol_size_chem_tanimoto_scores[key] / node_dist_dict[key]\n",
    "\n",
    "for key in variance_dist_dict.keys():\n",
    "    variance_context_errors[key] = variance_context_errors[key] / variance_dist_dict[key]\n",
    "    variance_shape_tanimoto_scores[key] = variance_shape_tanimoto_scores[key] / variance_dist_dict[key]\n",
    "    variance_chem_tanimoto_scores[key] = variance_chem_tanimoto_scores[key] / variance_dist_dict[key]\n",
    "\n",
    "\n",
    "with open(f\"generation_performance_report.txt\", \"w+\") as f:\n",
    "\n",
    "    f.write(f\"Number of Contexts used for generation - {len(dataset)}\\n\")\n",
    "    f.write(f\"Number of Samples per Context - {n_samples}\\n\\n\")\n",
    "    f.write(f\"Total valid molecules generated - {valid_samples} ({round(valid_samples_rate, 4) * 100}% out of requested)\\n\")\n",
    "    f.write(f\"From them, Chemically Unique in reference to training Dataset - {round(chem_unique_samples_rate, 4) * 100}%\\n\")\n",
    "    \n",
    "\n",
    "    f.write(\"\\nErrors in Context of Generated Molecules vs number of atoms in reference:\\n\\n\")\n",
    "    for key in sorted(ref_mol_size_context_errors.keys()):\n",
    "        f.write(f\"{key}:  {ref_mol_size_context_errors[key]}\\n\")\n",
    "\n",
    "    f.write(\"\\nShape Tanimoto Scores of Generated Molecules vs number of atoms in reference:\\n\\n\")\n",
    "    for key in sorted(ref_mol_size_shape_tanimoto_scores.keys()):\n",
    "        f.write(f\"{key}:  {ref_mol_size_shape_tanimoto_scores[key]}\\n\")\n",
    "\n",
    "     f.write(\"\\nChemical Tanimoto Scores of Generated Molecules vs number of atoms in reference:\\n\\n\")\n",
    "    for key in sorted(ref_mol_size_chem_tanimoto_scores.keys()):\n",
    "        f.write(f\"{key}:  {ref_mol_size_chem_tanimoto_scores[key]}\\n\")\n",
    "\n",
    "    f.write(\"\\nErrors in Context of Generated Molecules vs variation of number of atoms from reference:\\n\\n\")\n",
    "    for key in sorted(variance_context_errors.keys()):\n",
    "        f.write(f\"{key}:  {variance_context_errors[key]}\\n\")\n",
    "\n",
    "    f.write(\"\\nShape Tanimoto Scores of Generated Molecules vs variation of number of atoms from reference:\\n\\n\")\n",
    "    for key in sorted(variance_shape_tanimoto_scores.keys()):\n",
    "        f.write(f\"{key}:  {variance_shape_tanimoto_scores[key]}\\n\")\n",
    "\n",
    "    f.write(\"\\nChemical Tanimoto Scores of Generated Molecules vs variation of number of atoms from reference:\\n\\n\")\n",
    "    for key in sorted(variance_chem_tanimoto_scores.keys()):\n",
    "        f.write(f\"{key}:  {variance_chem_tanimoto_scores[key]}\\n\")\n",
    "        \n",
    "\n",
    "# - What is the error in context of generated samples vs number of atoms in the reference and variance\n",
    "# - What is the average shape tanimoto similarity of generated samples vs number of atoms in the reference and variance\n",
    "# - How many valid samples (after cheminformatics pipeline) was generated (as % from generated with edm) vs number of atoms in the reference\n",
    "# - How many chemically unique samples (which the model has never seen) was generated in total (as % of the number of all valid samples generated )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
